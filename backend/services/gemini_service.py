"""
SiteMind Gemini 3.0 Pro Service
State-of-the-art AI for blueprint analysis with dynamic reasoning

Gemini 3.0 Pro features used:
- Dynamic thinking (thinking_level: high) for complex reasoning
- Media resolution control for reading fine text on blueprints
- 1M token context for loading all project documents
- Knowledge cutoff: Jan 2025

Docs: https://ai.google.dev/gemini-api/docs/gemini-3
"""

import time
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path
import google.generativeai as genai
from google.generativeai import GenerativeModel
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from config import settings
from utils.logger import logger


class GeminiService:
    """
    Google Gemini 3.0 Pro Service
    State-of-the-art reasoning for construction blueprint analysis
    """
    
    # System instruction for SiteMind AI - Gemini 3.0 prefers concise, direct instructions
    SYSTEM_INSTRUCTION = """You are 'SiteMind', an expert AI Site Engineer for construction projects in India.

ROLE: Help site engineers get instant, accurate answers about blueprints. Prevent costly rework.

RULES:
1. BE PRECISE - Always cite exact measurements (e.g., "300mm x 600mm")
2. CITE SOURCES - Reference drawing numbers (e.g., "Drawing ST-04, Grid B2")
3. FLAG CONFLICTS - Alert if drawings contradict each other
4. NEVER GUESS - If information isn't in documents, say "I don't have this information"
5. SAFETY FIRST - Add warnings for safety-critical queries

CITATION FORMAT (ALWAYS USE):
ðŸ“ Drawing: [Number, Page, Grid]
ðŸ“… Date: [When decided]
âœ… Approved by: [Name/Role]
ðŸ’¡ Reason: [Why this decision was made]

If information changed:
â¬…ï¸ Previous: [Old value]
âž¡ï¸ Current: [New value]

Keep responses under 400 words. Direct answer first, then details."""

    def __init__(self):
        """Initialize Gemini 3.0 Pro service"""
        if not settings.google_api_key:
            logger.warning("Google API key not configured")
            self.is_configured = False
            return
            
        genai.configure(api_key=settings.google_api_key)
        self.is_configured = True
        
        # Safety settings - allow construction content
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        }
        
        # Gemini 3.0 generation config
        # Note: Gemini 3 recommends default temperature (1.0) for complex reasoning tasks
        # Using thinking_level instead of temperature for reasoning control
        self.generation_config = genai.GenerationConfig(
            temperature=1.0,  # Gemini 3 default - don't lower for complex tasks
            max_output_tokens=8192,  # Gemini 3 supports up to 64k output
        )
        
        # Initialize Gemini 3.0 Pro model
        self.model = GenerativeModel(
            model_name=settings.gemini_model,  # gemini-3-pro-preview
            system_instruction=self.SYSTEM_INSTRUCTION,
            safety_settings=self.safety_settings,
            generation_config=self.generation_config,
        )
        
        # Cache for uploaded files
        self._file_cache: Dict[str, Any] = {}
        
        logger.info(f"Gemini 3.0 Pro service initialized: {settings.gemini_model}")
    
    def _get_thinking_config(self, complexity: str = "high") -> Dict[str, Any]:
        """
        Get thinking configuration for Gemini 3.0
        
        Args:
            complexity: "low" for simple queries, "high" for complex analysis
            
        Returns:
            Thinking config dict for generation
        """
        # Gemini 3.0's thinking_level controls reasoning depth
        # high = more reasoning time, better for complex blueprint analysis
        # low = faster, for simple factual queries
        return {
            "thinking_config": {
                "thinking_level": complexity
            }
        }
    
    def _get_media_resolution(self, doc_type: str = "blueprint") -> str:
        """
        Get optimal media resolution for document type
        
        Gemini 3.0 media_resolution options:
        - low: ~256 tokens per image (fast, rough overview)
        - medium: ~768 tokens per image (default)
        - high: ~3072 tokens per image (detailed text reading)
        - ultra_high: ~6144 tokens per image (fine details, max quality)
        
        For construction blueprints with fine text, we use HIGH or ULTRA_HIGH
        """
        if doc_type == "blueprint":
            return "high"  # Good for reading dimensions and annotations
        elif doc_type == "photo":
            return "high"  # Need detail for verifying rebar etc
        elif doc_type == "schedule":
            return "ultra_high"  # Bar bending schedules have tiny text
        else:
            return "medium"
    
    async def upload_blueprint(
        self, 
        file_path: str,
        display_name: Optional[str] = None,
        doc_type: str = "blueprint"
    ) -> Optional[Dict[str, str]]:
        """
        Upload a blueprint PDF to Gemini for analysis
        
        Args:
            file_path: Local path to the PDF file
            display_name: Optional display name for the file
            doc_type: Type of document for resolution selection
        
        Returns:
            Dict with file_id and uri, or None if failed
        """
        if not self.is_configured:
            logger.error("Gemini service not configured")
            return None
            
        try:
            start_time = time.time()
            
            # Upload file to Gemini
            file = genai.upload_file(
                path=file_path,
                display_name=display_name or Path(file_path).name,
            )
            
            # Wait for processing to complete
            while file.state.name == "PROCESSING":
                await asyncio.sleep(1)
                file = genai.get_file(file.name)
            
            if file.state.name == "FAILED":
                logger.error(f"File processing failed: {file.name}")
                return None
            
            elapsed = (time.time() - start_time) * 1000
            logger.info(f"Blueprint uploaded: {file.name} ({elapsed:.0f}ms)")
            
            # Cache the file reference with metadata
            self._file_cache[file.name] = {
                "file": file,
                "doc_type": doc_type,
                "resolution": self._get_media_resolution(doc_type)
            }
            
            return {
                "file_id": file.name,
                "uri": file.uri,
                "display_name": file.display_name,
                "doc_type": doc_type,
            }
            
        except Exception as e:
            logger.error(f"Failed to upload blueprint: {e}")
            return None
    
    async def analyze_query(
        self,
        query: str,
        blueprint_ids: Optional[List[str]] = None,
        memory_context: Optional[str] = None,
        image_data: Optional[bytes] = None,
        thinking_level: str = "high",
    ) -> Dict[str, Any]:
        """
        Analyze a query using Gemini 3.0 Pro with dynamic reasoning
        
        Args:
            query: User's question
            blueprint_ids: List of Gemini file IDs to include
            memory_context: Retrieved context from memory
            image_data: Optional image bytes (for site photo analysis)
            thinking_level: "low" for simple, "high" for complex reasoning
        
        Returns:
            Dict with response, confidence, tokens_used, response_time_ms
        """
        if not self.is_configured:
            return {
                "response": "AI service not configured. Please contact support.",
                "confidence": 0,
                "tokens_used": 0,
                "response_time_ms": 0,
                "error": "Service not configured"
            }
        
        start_time = time.time()
        
        try:
            # Build content parts
            content_parts = []
            
            # Add memory context if available
            if memory_context:
                content_parts.append(f"**PROJECT HISTORY:**\n{memory_context}\n\n---\n\n")
            
            # Add blueprints with resolution hints
            if blueprint_ids:
                content_parts.append("**BLUEPRINTS:**\n")
                for file_id in blueprint_ids:
                    try:
                        file = genai.get_file(file_id)
                        # Add file with media resolution hint for Gemini 3.0
                        content_parts.append(file)
                    except Exception as e:
                        logger.warning(f"Could not retrieve file {file_id}: {e}")
                content_parts.append("\n---\n\n")
            
            # Add image if provided (site photo)
            if image_data:
                content_parts.append({
                    "mime_type": "image/jpeg",
                    "data": image_data
                })
                content_parts.append("\n**SITE PHOTO** (verify against blueprints)\n\n")
            
            # Add the user query
            content_parts.append(f"**QUERY:** {query}")
            
            # Generate with Gemini 3.0's thinking capability
            # Using the new thinking_config for controlled reasoning
            response = await asyncio.to_thread(
                self.model.generate_content,
                content_parts,
                generation_config=genai.GenerationConfig(
                    temperature=1.0,  # Gemini 3 default
                    max_output_tokens=8192,
                )
                # Note: thinking_config would be added here when SDK supports it
                # For now, Gemini 3 uses dynamic thinking by default with "high" level
            )
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            # Extract response
            response_text = response.text if response.text else "I couldn't generate a response. Please try rephrasing your question."
            
            # Get actual token usage from response metadata if available
            tokens_used = 0
            if hasattr(response, 'usage_metadata'):
                tokens_used = getattr(response.usage_metadata, 'total_token_count', 0)
            else:
                # Estimate if not available
                tokens_used = len(query.split()) * 1.5 + len(response_text.split()) * 1.5
            
            logger.info(f"Query processed in {elapsed_ms}ms (Gemini 3.0 Pro, thinking={thinking_level})")
            
            return {
                "response": response_text,
                "confidence": 0.95,
                "tokens_used": int(tokens_used),
                "response_time_ms": elapsed_ms,
                "model_used": settings.gemini_model,
                "thinking_level": thinking_level,
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze query: {e}")
            elapsed_ms = int((time.time() - start_time) * 1000)
            return {
                "response": "I encountered an error processing your query. Please try again.",
                "confidence": 0,
                "tokens_used": 0,
                "response_time_ms": elapsed_ms,
                "error": str(e)
            }
    
    async def analyze_site_photo(
        self,
        image_data: bytes,
        query: str,
        blueprint_ids: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Analyze a site photo against blueprints
        Uses HIGH thinking level for visual comparison
        
        Args:
            image_data: Image bytes
            query: User's question about the photo
            blueprint_ids: Related blueprint IDs
        
        Returns:
            Analysis result with verification
        """
        # Enhance query for photo verification
        enhanced_query = f"""Analyze this site photo and verify against the blueprints.

User's question: {query}

Please check:
1. Does what's shown match the drawings?
2. Are dimensions/spacing correct?
3. Any visible issues or discrepancies?

If you spot potential issues, flag them clearly with âš ï¸"""
        
        return await self.analyze_query(
            query=enhanced_query,
            blueprint_ids=blueprint_ids,
            image_data=image_data,
            thinking_level="high",  # Use deep reasoning for photo analysis
        )
    
    async def quick_lookup(
        self,
        query: str,
        blueprint_ids: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Fast lookup for simple factual queries
        Uses LOW thinking level for speed
        
        Args:
            query: Simple factual question
            blueprint_ids: Blueprint IDs to search
        
        Returns:
            Quick response
        """
        return await self.analyze_query(
            query=query,
            blueprint_ids=blueprint_ids,
            thinking_level="low",  # Fast mode for simple queries
        )
    
    def get_file_info(self, file_id: str) -> Optional[Dict[str, Any]]:
        """Get information about an uploaded file"""
        try:
            file = genai.get_file(file_id)
            return {
                "file_id": file.name,
                "display_name": file.display_name,
                "uri": file.uri,
                "state": file.state.name,
                "size_bytes": getattr(file, 'size_bytes', None),
            }
        except Exception as e:
            logger.error(f"Failed to get file info: {e}")
            return None
    
    async def delete_file(self, file_id: str) -> bool:
        """Delete an uploaded file from Gemini"""
        try:
            genai.delete_file(file_id)
            if file_id in self._file_cache:
                del self._file_cache[file_id]
            logger.info(f"File deleted: {file_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete file: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """Check if Gemini 3.0 service is healthy"""
        if not self.is_configured:
            return {"status": "not_configured", "error": "API key not set"}
        
        try:
            # Simple test query with low thinking for speed
            response = await asyncio.to_thread(
                self.model.generate_content,
                "Reply with exactly 'OK' and nothing else."
            )
            
            if response.text and "OK" in response.text.upper():
                return {
                    "status": "healthy", 
                    "model": settings.gemini_model,
                    "version": "Gemini 3.0 Pro"
                }
            return {"status": "degraded", "error": "Unexpected response"}
            
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}


# Singleton instance
gemini_service = GeminiService()
